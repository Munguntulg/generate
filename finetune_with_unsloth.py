#!/usr/bin/env python3
"""
Unsloth –∞—à–∏–≥–ª–∞–Ω qwen2.5:7b fine-tune —Ö–∏–π—Ö

–°—É—É–ª–≥–∞—Ö:
pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
pip install --no-deps "xformers<0.0.27" "trl<0.9.0" peft accelerate bitsandbytes
"""

import json
from pathlib import Path
from unsloth import FastLanguageModel
from datasets import Dataset
from trl import SFTTrainer
from transformers import TrainingArguments
import torch


def load_training_data(filepath: str) -> list:
    """”®–≥”©–≥–¥”©–ª —É–Ω—à–∞—Ö"""
    with open(filepath, 'r', encoding='utf-8') as f:
        data = json.load(f)
    return data.get('samples', [])


def format_sample(sample: dict) -> str:
    """
    Sample-–≥ chat format –±–æ–ª–≥–æ—Ö
    
    Qwen2.5 format:
    <|im_start|>system
    {system_prompt}<|im_end|>
    <|im_start|>user
    {user_message}<|im_end|>
    <|im_start|>assistant
    {assistant_response}<|im_end|>
    """
    
    system_prompt = """–¢–∞ –ø—Ä–æ—Ç–æ–∫–æ–ª–æ–æ—Å —è—Ä–∏–∞–Ω—ã —Ö—ç–ª–∏–π–≥ –∞–ª–±–∞–Ω —Ö—ç–ª –±–æ–ª–≥–æ–¥–æ–≥ –º—ç—Ä–≥—ç–∂–∏–ª—Ç—ç–Ω.

–ß–£–•–ê–õ –î“Æ–†–≠–ú:
1. –ê–ì–£–£–õ–ì–ê ”®”®–†–ß–õ”®–•–ì“Æ–ô (–Ω—ç—Ä, –æ–≥–Ω–æ–æ, —Ç–æ–æ)
2. –•—ç–ª–ª—ç–≥ “Ø–≥—Å –ê–†–ò–õ–ì–ê (—à“Ø“Ø –¥—ç—ç, –ª –±–∞–π—Ö –¥–∞–∞)
3. “Æ–π–ª “Ø–≥ –∞–ª–±–∞–Ω —Ö—ç–ª –±–æ–ª–≥–æ (—Ö–∏–π—Ö ‚Üí –≥“Ø–π—Ü—ç—Ç–≥—ç—Ö)

–ó”©–≤—Ö”©–Ω –∞–ª–±–∞–Ω —Ö—É–≤–∏–ª–±–∞—Ä –±—É—Ü–∞–∞."""
    
    user_message = f"–≠–Ω—ç —Ç–µ–∫—Å—Ç–∏–π–≥ –∞–ª–±–∞–Ω –ø—Ä–æ—Ç–æ–∫–æ–ª –±–æ–ª–≥–æ:\n\n{sample['input']}"
    assistant_response = sample['output']
    
    formatted = f"""<|im_start|>system
{system_prompt}<|im_end|>
<|im_start|>user
{user_message}<|im_end|>
<|im_start|>assistant
{assistant_response}<|im_end|>"""
    
    return formatted


def prepare_dataset(samples: list) -> Dataset:
    """Dataset –±—ç–ª—Ç–≥—ç—Ö"""
    formatted_texts = [format_sample(s) for s in samples]
    
    dataset_dict = {
        "text": formatted_texts,
        "id": [s['id'] for s in samples]
    }
    
    return Dataset.from_dict(dataset_dict)


def finetune_model(
    train_file: str = "data/processed/train.json",
    val_file: str = "data/processed/validation.json",
    output_dir: str = "models/qwen2.5-mongolian-protocol",
    max_seq_length: int = 2048,
    learning_rate: float = 2e-4,
    num_epochs: int = 3,
    batch_size: int = 4
):
    """
    Fine-tuning —Ö–∏–π—Ö
    """
    
    print(f"\n{'='*60}")
    print(f"FINE-TUNING –≠–•–õ“Æ“Æ–õ–ñ –ë–ê–ô–ù–ê")
    print(f"{'='*60}\n")
    
    # 1. Model –∞—á–∞–∞–ª–∞—Ö
    print(f"1Ô∏è‚É£  Model –∞—á–∞–∞–ª–∂ –±–∞–π–Ω–∞...")
    print(f"   Base model: qwen2.5:7b")
    print(f"   Max sequence: {max_seq_length}")
    
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name="Qwen/Qwen2.5-7B-Instruct",  # HuggingFace model –Ω—ç—Ä
        max_seq_length=max_seq_length,
        dtype=None,  # Auto detect
        load_in_4bit=True,  # 4-bit quantization (memory —Ö—ç–º–Ω—ç—Ö)
    )
    
    print(f"   ‚úì Model –∞—á–∞–∞–ª–∞–≥–¥—Å–∞–Ω\n")
    
    # 2. LoRA config
    print(f"2Ô∏è‚É£  LoRA —Ç–æ—Ö–∏—Ä—É—É–ª–∂ –±–∞–π–Ω–∞...")
    
    model = FastLanguageModel.get_peft_model(
        model,
        r=16,  # LoRA rank
        target_modules=[
            "q_proj", "k_proj", "v_proj", "o_proj",
            "gate_proj", "up_proj", "down_proj"
        ],
        lora_alpha=16,
        lora_dropout=0.05,
        bias="none",
        use_gradient_checkpointing="unsloth",
        random_state=42,
    )
    
    print(f"   ‚úì LoRA ready\n")
    
    # 3. Dataset –±—ç–ª—Ç–≥—ç—Ö
    print(f"3Ô∏è‚É£  ”®–≥”©–≥–¥”©–ª –±—ç–ª—Ç–≥—ç–∂ –±–∞–π–Ω–∞...")
    
    train_samples = load_training_data(train_file)
    val_samples = load_training_data(val_file)
    
    train_dataset = prepare_dataset(train_samples)
    val_dataset = prepare_dataset(val_samples)
    
    print(f"   ‚úì Train: {len(train_dataset)} –∂–∏—à—ç—ç")
    print(f"   ‚úì Val:   {len(val_dataset)} –∂–∏—à—ç—ç\n")
    
    # 4. Training arguments
    print(f"4Ô∏è‚É£  Training —Ç–æ—Ö–∏—Ä–≥–æ–æ...")
    
    training_args = TrainingArguments(
        output_dir=output_dir,
        per_device_train_batch_size=batch_size,
        per_device_eval_batch_size=batch_size,
        gradient_accumulation_steps=4,
        warmup_steps=10,
        num_train_epochs=num_epochs,
        learning_rate=learning_rate,
        fp16=not torch.cuda.is_bf16_supported(),
        bf16=torch.cuda.is_bf16_supported(),
        logging_steps=10,
        eval_strategy="steps",
        eval_steps=50,
        save_strategy="steps",
        save_steps=100,
        save_total_limit=3,
        load_best_model_at_end=True,
        optim="adamw_8bit",
        weight_decay=0.01,
        lr_scheduler_type="cosine",
        seed=42,
    )
    
    print(f"   ‚úì Epochs: {num_epochs}")
    print(f"   ‚úì Batch size: {batch_size}")
    print(f"   ‚úì Learning rate: {learning_rate}\n")
    
    # 5. Trainer
    print(f"5Ô∏è‚É£  Trainer “Ø“Ø—Å–≥—ç–∂ –±–∞–π–Ω–∞...")
    
    trainer = SFTTrainer(
        model=model,
        tokenizer=tokenizer,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        dataset_text_field="text",
        max_seq_length=max_seq_length,
        args=training_args,
    )
    
    print(f"   ‚úì Trainer –±—ç–ª—ç–Ω\n")
    
    # 6. FINE-TUNING –≠–•–õ–≠–•
    print(f"{'='*60}")
    print(f"‚è≥ TRAINING –≠–•–≠–õ–ñ –ë–ê–ô–ù–ê...")
    print(f"{'='*60}\n")
    
    trainer.train()
    
    print(f"\n{'='*60}")
    print(f"‚úÖ TRAINING –î–£–£–°–õ–ê–ê!")
    print(f"{'='*60}\n")
    
    # 7. Model —Ö–∞–¥–≥–∞–ª–∞—Ö
    print(f"6Ô∏è‚É£  Model —Ö–∞–¥–≥–∞–ª–∂ –±–∞–π–Ω–∞...")
    
    model.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)
    
    print(f"   ‚úì –•–∞–¥–≥–∞–ª—Å–∞–Ω: {output_dir}\n")
    
    # 8. Ollama-–¥ export —Ö–∏–π—Ö
    print(f"7Ô∏è‚É£  Ollama format —Ä—É—É —Ö”©—Ä–≤“Ø“Ø–ª–∂ –±–∞–π–Ω–∞...")
    
    # GGUF format (Ollama-–¥ –∑–æ—Ä–∏—É–ª–∂)
    model.save_pretrained_gguf(
        f"{output_dir}/gguf",
        tokenizer,
        quantization_method="q4_k_m"  # 4-bit quantization
    )
    
    print(f"   ‚úì GGUF export: {output_dir}/gguf\n")
    
    print(f"{'='*60}")
    print(f"üéâ –ë“Æ–ì–î –ê–ú–ñ–ò–õ–¢–¢–ê–ô!")
    print(f"{'='*60}\n")
    
    print(f"–î–∞—Ä–∞–∞–≥–∏–π–Ω –∞–ª—Ö–∞–º:")
    print(f"1. Ollama Modelfile “Ø“Ø—Å–≥—ç—Ö:")
    print(f"   cat > Modelfile << 'EOF'")
    print(f"   FROM {output_dir}/gguf/model.gguf")
    print(f"   TEMPLATE [... template ...]")
    print(f"   EOF")
    print(f"\n2. Ollama-–¥ import:")
    print(f"   ollama create mongolian-protocol -f Modelfile")
    print(f"\n3. –¢–µ—Å—Ç–ª—ç—Ö:")
    print(f"   ollama run mongolian-protocol\n")


if __name__ == "__main__":
    # GPU —à–∞–ª–≥–∞—Ö
    if torch.cuda.is_available():
        print(f"‚úÖ GPU: {torch.cuda.get_device_name(0)}")
        print(f"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\n")
    else:
        print(f"‚ö†Ô∏è  GPU –±–∞–π—Ö–≥“Ø–π - CPU –∞—à–∏–≥–ª–∞–Ω–∞ (—É–¥–∞–∞–Ω –±–æ–ª–Ω–æ)\n")
    
    # Fine-tuning —ç—Ö–ª“Ø“Ø–ª—ç—Ö
    finetune_model(
        train_file="data/processed/train.json",
        val_file="data/processed/validation.json",
        output_dir="models/qwen2.5-mongolian-protocol",
        num_epochs=3,
        batch_size=2 if not torch.cuda.is_available() else 4
    )