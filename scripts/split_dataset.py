#!/usr/bin/env python3
"""
”®–≥”©–≥–¥–ª–∏–π–≥ Train/Val/Test —Ö—É–≤–∞–∞—Ö
80% train, 10% validation, 10% test
"""

import json
import random
from pathlib import Path


def split_dataset(
    input_file: str,
    output_dir: str = "data/processed",
    train_ratio: float = 0.8,
    val_ratio: float = 0.1,
    seed: int = 42
):
    """
    ”®–≥”©–≥–¥–ª–∏–π–≥ —Ö—É–≤–∞–∞—Ö
    
    Args:
        input_file: –û—Ä–æ—Ö —Ñ–∞–π–ª
        output_dir: –ì–∞—Ä–∞—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä
        train_ratio: Train —Ö—É–≤—å (0.8 = 80%)
        val_ratio: Validation —Ö—É–≤—å (0.1 = 10%)
        seed: Random seed (reproducibility)
    """
    
    print(f"\n{'='*60}")
    print(f"”®–ì”®–ì–î”®–õ –•–£–í–ê–ê–•")
    print(f"{'='*60}\n")
    
    # ”®–≥”©–≥–¥”©–ª —É–Ω—à–∞—Ö
    print(f"1Ô∏è‚É£  ”®–≥”©–≥–¥”©–ª —É–Ω—à–∏–∂ –±–∞–π–Ω–∞...")
    with open(input_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    samples = data.get('samples', [])
    print(f"   ‚úì {len(samples)} –∂–∏—à—ç—ç –æ–ª–¥—Å–æ–Ω\n")
    
    # Shuffle
    print(f"2Ô∏è‚É£  –•–æ–ª–∏—Ö (seed={seed})...")
    random.seed(seed)
    random.shuffle(samples)
    
    # –•—É–≤–∞–∞—Ö
    total = len(samples)
    train_size = int(total * train_ratio)
    val_size = int(total * val_ratio)
    
    train_samples = samples[:train_size]
    val_samples = samples[train_size:train_size + val_size]
    test_samples = samples[train_size + val_size:]
    
    print(f"   ‚úì Train: {len(train_samples)} ({len(train_samples)/total*100:.1f}%)")
    print(f"   ‚úì Val:   {len(val_samples)} ({len(val_samples)/total*100:.1f}%)")
    print(f"   ‚úì Test:  {len(test_samples)} ({len(test_samples)/total*100:.1f}%)\n")
    
    # –•–∞–¥–≥–∞–ª–∞—Ö
    print(f"3Ô∏è‚É£  –•–∞–¥–≥–∞–ª–∂ –±–∞–π–Ω–∞...")
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    splits = {
        "train": train_samples,
        "validation": val_samples,
        "test": test_samples
    }
    
    for split_name, split_samples in splits.items():
        filename = output_path / f"{split_name}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump({"samples": split_samples}, f, ensure_ascii=False, indent=2)
        
        print(f"   ‚úì {filename} ({len(split_samples)} –∂–∏—à—ç—ç)")
    
    print(f"\n{'='*60}")
    print(f"‚úÖ –ê–ú–ñ–ò–õ–¢–¢–ê–ô!")
    print(f"{'='*60}\n")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫
    print(f"–§–∞–π–ª—É—É–¥:")
    for split_name in ["train", "validation", "test"]:
        filepath = output_path / f"{split_name}.json"
        size_kb = filepath.stat().st_size / 1024
        print(f"  üìÅ {filepath}")
        print(f"     –•—ç–º–∂—ç—ç: {size_kb:.1f} KB\n")


if __name__ == "__main__":
    split_dataset(
        input_file="data/raw/expanded_dataset.json",
        output_dir="data/processed"
    )